<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<documents>
    <document uid="862f0b73a8aea37d41ce6b819175aa09">
        <field name="title"><![CDATA[Orchestrierung mit Nomad]]></field>
        <field name="subline"><![CDATA[]]></field>
        <field name="teaser"><![CDATA[<p>Orchestrierung ist in aller Munde und aus vielen Bereichen gar nicht mehr wegzudenken - doch gibt es
neben dem Platzhirschen Kubernetes eigentlich Alternativen?
Im Zuge dieses Artikels beschäftigen wir uns mit dem Job Scheduler Nomad aus dem Hause
HashiCorp und sehen uns anhand von einfachen Beispielen an, welche Möglichkeiten hier geboten
werden.
Anschließend beschäftigen wir uns mit Deployments und fortgeschritteneren Themen wie Service
Discovery und Canary Deployments.</p>

]]></field>
        <field name="language_multi_keyword"><![CDATA[de]]></field>
        <field name="content_type_multi_keyword"><![CDATA[blog]]></field>
        <field name="mime_type_multi_keyword"><![CDATA[text/html]]></field>
        <field name="category_multi_keyword"><![CDATA[Softwareentwicklung]]></field>
        <field name="tag_multi_keyword"><![CDATA[Orchestrierung]]></field>
        <field name="tag_multi_keyword"><![CDATA[Kubernetes]]></field>
        <field name="tag_multi_keyword"><![CDATA[Nomad]]></field>
        <field name="tag_multi_keyword"><![CDATA[Consul]]></field>
        <field name="tag_multi_keyword"><![CDATA[Fabio]]></field>
        <field name="date_date"><![CDATA[2023-02-13T11:00:00+01:00]]></field>
        <field name="date_l"><![CDATA[1676282400000]]></field>
        <field name="change_date"><![CDATA[1676282400000]]></field>

        <!--Author Information-->
        
        <field name="author_id"><![CDATA[unexist]]></field><!--Postcontent-->
        <field name="headlines"><![CDATA[Orchestrierung mit Nomad]]></field>
        <field name="display_content"><![CDATA[<div class="i2-intro p-t-1">
            <p>Orchestrierung ist in aller Munde und aus vielen Bereichen gar nicht mehr wegzudenken - doch gibt es
neben dem Platzhirschen Kubernetes eigentlich Alternativen?
Im Zuge dieses Artikels beschäftigen wir uns mit dem Job Scheduler Nomad aus dem Hause
HashiCorp und sehen uns anhand von einfachen Beispielen an, welche Möglichkeiten hier geboten
werden.
Anschließend beschäftigen wir uns mit Deployments und fortgeschritteneren Themen wie Service
Discovery und Canary Deployments.</p>

</div>]]></field>
        <field name="content"><![CDATA[<div class="adesso-text-formate">
<div class="row p-t-2">
<div class="adesso-container">
<div class="col-xl-8 adesso-center p-b-1 p-l-0 p-r-0">
    <p>Orchestrierung ist in aller Munde und aus vielen Bereichen gar nicht mehr wegzudenken - doch gibt es
neben dem Platzhirschen Kubernetes eigentlich Alternativen?
Im Zuge dieses Artikels beschäftigen wir uns mit dem Job Scheduler Nomad aus dem Hause
HashiCorp und sehen uns anhand von einfachen Beispielen an, welche Möglichkeiten hier geboten
werden.
Anschließend beschäftigen wir uns mit Deployments und fortgeschritteneren Themen wie Service
Discovery und Canary Deployments.</p>

<h4 id="was-genau-ist-nomad">Was genau ist Nomad?</h4>

<p>Bei <a href="https://www.nomadproject.io">Nomad</a> handelt es sich um einen kleinen Job Scheduler und Orchestrator, der im Gegensatz zu
<a href="https://kubernetes.io/">Kubernetes</a> nicht nur Container, sondern über Plugins im Grunde alles mögliche verwalten kann.</p>

<p>Dies wird über Task Driver realisiert, die sowohl von der Community beigesteuert werden aber
auch direkt von Hause aus mit dabei sind.
Darunter befinden sich neben üblichen Verdächtigen wie <a href="https://www.nomadproject.io/docs/drivers/docker">Docker</a> und <a href="https://developer.hashicorp.com/nomad/plugins/drivers/podman">Podman</a> auch
beispielsweise ein <a href="https://www.nomadproject.io/docs/internals/plugins/task-drivers">Task Driver</a> speziell für <a href="https://www.nomadproject.io/docs/drivers/java">Java-Anwendungen</a> sowie <a href="https://www.nomadproject.io/docs/drivers/raw_exec">Raw/Exec</a> für
alle sonstigen ausführbare Anwendungen.</p>

<p>Bevor wir jetzt in ein Beispiel einsteigen, sollten wir kurz über die Konfiguration sprechen.</p>

<h4 id="konfiguration-ohne-yaml">Konfiguration ohne YAML</h4>

<p>Im Gegensatz zum rein deklarativen Ansatz von Kubernetes, bei dem <a href="https://yaml.org/">YAML</a> Dateien den
gewünschten Zielzustand beschreiben, verwendet Nomad die hauseigene Konfigurationssprache
<a href="https://github.com/hashicorp/hcl">HCL</a>.
Ursprünglich für den Einsatz bei <a href="https://terraform.io">Terraform</a> erdacht bringt sie Logikfunktionen und Operatoren
mit, über die auch komplexe Szenarien abgebildet werden können, sodass ein Einsatz von weiteren
Hilfsmitteln wie <a href="https://kustomize.io/">kustomize</a> nicht erforderlich ist.</p>

<p>Natürlich steht mit <a href="https://learn.hashicorp.com/tutorials/nomad/nomad-pack-intro">Nomad Pack</a> ein vergleichbares Pendant zu <a href="https://helm.se">Helm</a> zur Verfügung, sodass
wir auch hier versionierte Artefakte erstellen können.</p>

<p>Hier dazu ein einfaches Beispiel zu HCL:</p>

<pre><code class="language-hcl">company = "adesso"
message = "Hallo ${company}"
loud_message = upper(message)
colors = [ "red", "blue" ]
options = {
  color: element(colors, 1),
  amount: 100
}

configuration {
  service "greeter" {
    message = loud_message
    options = var.override_options ? var.override_options : var.options
  }
}
</code></pre>

<blockquote>
  <p><strong><em>NOTE:</em></strong> Eine vollständige Dokumentation findest du auf der <a href="https://github.com/hashicorp/hcl/blob/main/hclsyntax/spec.md">offiziellen Projektseite</a>.</p>
</blockquote>

<h4 id="alles-über-jobs">Alles über Jobs</h4>

<p>Jobs stellen in Nomad die eigentliche Arbeitseinheit dar und können über verschiedene Wege
an den Server geschickt werden - dazu aber später mehr.</p>

<p>Grundsätzlich läuft die Einreichung eines Jobs wie folgt ab:
Nach erfolgter Übermittlung findet zunächst die <a href="https://developer.hashicorp.com/nomad/docs/concepts/scheduling/scheduling">Evaluation</a> statt und es werden die notwendigen
Schritte erfasst und aufbereitet.
Anschließend startet die <a href="https://www.nomadproject.io/docs/concepts/scheduling/scheduling">Allocation</a>, hierbei wird ein Ausführplan erstellt und eine
<a href="https://www.nomadproject.io/docs/job-specification/group">Task Group</a> angelegt und somit auch die Zuweisung eines Jobs zu einem aktiven Client erstellt.</p>

<blockquote>
  <p><strong><em>NOTE:</em></strong> Diese Schritte sollte man zumindest einmal gehört haben, denn sie begegnen einem
durchaus als Status im täglichen Umgang mit Nomad.</p>
</blockquote>

<h5 id="wie-sieht-ein-job-aus">Wie sieht ein Job aus?</h5>

<p><a href="https://www.nomadproject.io/docs/job-specification/job">Jobs</a> bzw. die eigentlichen <a href="https://www.nomadproject.io/docs/job-specification/resources">Job-Files</a> bestehen aus verschiedensten Objekten (auch
<a href="https://en.wikipedia.org/wiki/Stanza">Stanza</a> genannt) und lassen sich am einfachsten an konkreten Beispielen erklären.</p>

<p>Als groben Rahmen für die nächsten Abschnitte nutzen wir eine <a href="https://quarkus.io">Quarkus</a>-basierte
Backendanwendung, mit der über eine einfache <a href="https://en.wikipedia.org/wiki/Representational_state_transfer">REST</a> API Todo-Einträge erstellt und verwaltet
werden können.</p>

<blockquote>
  <p><strong><em>NOTE:</em></strong> Die passende <a href="https://www.openapis.org/">OpenAPI</a>-Spezifikation zum Beispiel findet ihr <a href="https://blog.unexist.dev/redoc/">hier</a>.</p>
</blockquote>

<p>Hier jetzt aber unser erstes Job-File:</p>

<pre><code class="language-hcl">job "todo" {
  datacenters = ["dc1"] # &lt;1&gt;

  group "web" { # &lt;2&gt;
    count = 1 # &lt;3&gt;

    task "todo" { # &lt;4&gt;
      driver = "java" # &lt;5&gt;

      config { # &lt;6&gt;
        jar_path = "/Users/christoph.kappel/Projects/showcase-nomad-quarkus/target/showcase-nomad-quarkus-0.1-runner.jar"
        jvm_options = ["-Xmx256m", "-Xms256m"]
      }

      resources { # &lt;7&gt;
        memory = 256
      }
    }

    network { # &lt;8&gt;
      port "http" `
        static = 8080
      }
    }
  }
}
</code></pre>
<p><strong>&lt;1&gt;</strong> Nomad teilt Clients in Datacenter auf - ist somit Datacenter-aware.<br />
<strong>&lt;2&gt;</strong> Gruppen können aus verschiedenen Tasks bestehen und werden stets auf demselben Client ausgeführt.<br />
<strong>&lt;3&gt;</strong> Hier starten wir maximal eine Instanz dieser Gruppe.<br />
<strong>&lt;4&gt;</strong> Ein Task stellt die kleinste Einheit in Nomad dar - vergleichbar mit einem <a href="https://kubernetes.io/docs/concepts/workloads/pods/">Pod</a>.<br />
<strong>&lt;5&gt;</strong> Der <a href="https://www.nomadproject.io/docs/drivers/java">Java</a> Task Driver startet ein Jar in einer <a href="https://en.wikipedia.org/wiki/Java_virtual_machine">JVM</a>-Instanz.<br />
<strong>&lt;6&gt;</strong> Die meisten Task Driver können konfiguriert werden - hier setzen wir JVM-Optionen.<br />
<strong>&lt;7&gt;</strong> <a href="https://developer.hashicorp.com/nomad/docs/job-specification/resources">Resource Limits</a> können ebenfalls gesetzt werden.<br />
<strong>&lt;8&gt;</strong> Und abschließend setzen wir noch den Netzwerkport - diesen brauchen wir später.<br /></p>

<blockquote>
  <p><strong><em>NOTE:</em></strong> Für die nächsten Schritte benötigst du eine laufende Nomad-Instanz - solltest du
hierbei noch Probleme haben, wirf am besten einen Blick in die <a href="https://learn.hashicorp.com/tutorials/nomad/get-started-intro">offizielle Anleitung</a>.</p>
</blockquote>

<h5 id="wie-reiche-ich-einen-job-ein">Wie reiche ich einen Job ein?</h5>

<p>Für die meisten Aktionen bei Nomad stehen folgende drei Wege zur Verfügung:</p>

<h6 id="via-browser">Via Browser</h6>

<p>Die einfachste Möglichkeit, eine Aktion durchzuführen, ist über den Browser und das mitgelieferte Webinterfaces, welches
direkt nach dem Start von Nomad unter folgender Adresse erreichbar ist: <a href="http://locahost:4646">http://locahost:4646</a></p>

<p><img src="/assets/images/posts/orchestrierung-mit-nomad/web.png" alt="image" /></p>

<p>Über den Knopf <strong>Run Job</strong> oben rechts gelangst du zu einem Dialog, in den du deine Job-Definition
direkt entweder mittels HCL oder <a href="https://www.json.org/json-en.html">JSON</a> abschicken kannst.</p>

<p>Mittels <strong>Plan</strong> wird ein Dry-Run ausgeführt und das Ergebnis unmittelbar angezeigt:</p>

<p><img src="/assets/images/posts/orchestrierung-mit-nomad/plan_success.png" alt="image" /></p>

<p>Und abschließend startet <strong>Run</strong> dann das finale Deployment:</p>

<p><img src="/assets/images/posts/orchestrierung-mit-nomad/job_success.png" alt="image" /></p>

<h6 id="über-die-kommandozeile">Über die Kommandozeile</h6>

<p>Für die <a href="https://en.wikipedia.org/wiki/Command-line_interface">Commandline</a>-Liebhaber unter euch bietet Nomad natürlich auch hier eine <a href="https://en.wikipedia.org/wiki/Command-line_interface">CLI</a>:</p>

<pre><code class="language-bash">$ nomad job plan jobs/todo-java.nomad
+ Job: "todo"
+ Task Group: "web" (1 create)
  + Task: "todo" (forces create)

Scheduler dry-run:
- All tasks successfully allocated.

$ nomad job run jobs/todo-java.nomad
==&gt; 2022-07-18T17:48:36+02:00: Monitoring evaluation "2c21d49b"
    2022-07-18T17:48:36+02:00: Evaluation triggered by job "todo"
==&gt; 2022-07-18T17:48:37+02:00: Monitoring evaluation "2c21d49b"
    2022-07-18T17:48:37+02:00: Evaluation within deployment: "83abca16"
    2022-07-18T17:48:37+02:00: Allocation "d9ec1c42" created: node "d419df0b", group "web"
    2022-07-18T17:48:37+02:00: Evaluation status changed: "pending" -&gt; "complete"
==&gt; 2022-07-18T17:48:37+02:00: Evaluation "2c21d49b" finished with status "complete"
==&gt; 2022-07-18T17:48:37+02:00: Monitoring deployment "83abca16"
  ✓ Deployment "83abca16" successful

    2022-07-18T17:48:47+02:00
    ID          = 83abca16
    Job ID      = todo
    Job Version = 0
    Status      = successful
    Description = Deployment completed successfully

    Deployed
    Task Group  Desired  Placed  Healthy  Unhealthy  Progress Deadline
    web         1        1       1        0          2022-07-18T17:58:46+02:00
</code></pre>

<blockquote>
  <p><strong><em>NOTE:</em></strong> Die verwendeten Nomad-Beispiel könnt ihr auch direkt hier finden:</p>
</blockquote>
<p><a href="https://github.com/unexist/showcase-nomad-quarkus/tree/master/deployment/jobs">https://github.com/unexist/showcase-nomad-quarkus/tree/master/deployment/jobs</a></p>

<h6 id="über-die-api">Über die API</h6>

<p>Und analog zu <a href="https://kubernetes.io/">Kubernetes</a> können wir Nomad auch direkt über die <a href="https://www.nomadproject.io/api-docs/jobs">Job API</a> ansprechen -
beispielsweise mittels <a href="https://curl.se/">curl</a>:</p>

<pre><code class="language-bash">$ curl --request POST --data @jobs/todo-java.json http://localhost:4646/v1/jobs
{"EvalCreateIndex":228,"EvalID":"bd809b77-e2c6-c336-c5ca-0d1c15ff6cce","Index":228,"JobModifyIndex":228,"KnownLeader":false,"LastContact":0,"NextToken":"","Warnings":""}
</code></pre>

<blockquote>
  <p><strong><em>NOTE:</em></strong> Und die entsprechende <a href="https://www.json.org/json-en.html">JSON</a>-Variante findet ihr schließlich hier:</p>
</blockquote>
<p><a href="https://github.com/unexist/showcase-nomad-quarkus/blob/master/deployment/jobs/todo-java.json">https://github.com/unexist/showcase-nomad-quarkus/blob/master/deployment/jobs/todo-java.json</a></p>

<p>Alle genannten Wege übermitteln unseren Job an Nomad und starten dann eine einzelne Instanz auf
einem Client des <a href="https://developer.hashicorp.com/nomad/docs/concepts/architecture">Datacenters</a> <code>dc1</code>.</p>

<h5 id="status-eines-jobs-überprüfen">Status eines Jobs überprüfen</h5>

<p>Auskunft über den Status eines Jobs bekommen wir direkt über das Webinterface, aber natürlich können wir in
auch in gewohnter Weise den Status unseres Jobs über die <a href="https://en.wikipedia.org/wiki/Command-line_interface">Commandline</a> erfragen:</p>

<pre><code class="language-bash">$ nomad job status
ID    Type     Priority  Status   Submit Date
todo  service  50        running  2022-07-18T17:48:36+02:00
</code></pre>

<p>Alternativ können wir natürlich auch direkt auf unseren <a href="https://en.wikipedia.org/wiki/Representational_state_transfer">REST</a> Service zugreifen - beispielsweise
erneut via <a href="https://curl.se/">curl</a>:</p>

<pre><code class="language-bash">$ curl -v -H "Accept: application/json" http://localhost:8080/todo
*   Trying ::1...
* TCP_NODELAY set
* Connected to localhost (::1) port 8080 (#0)
&gt; GET /todo HTTP/1.1
&gt; Host: localhost:8080
&gt; User-Agent: curl/7.64.1
&gt; Accept: application/json
`
&lt; HTTP/1.1 204 No Content
&lt;
* Connection #0 to host localhost left intact
* Closing connection 0
</code></pre>

<h5 id="jobs-stoppen">Jobs stoppen</h5>

<p>Und ebenso leicht können wir unseren Job auch wieder stoppen:</p>

<pre><code class="language-bash">$ nomad job stop todo
==&gt; 2022-07-18T18:04:55+02:00: Monitoring evaluation "efe42497"
    2022-07-18T18:04:55+02:00: Evaluation triggered by job "todo"
==&gt; 2022-07-18T18:04:56+02:00: Monitoring evaluation "efe42497"
    2022-07-18T18:04:56+02:00: Evaluation within deployment: "577c3e71"
    2022-07-18T18:04:56+02:00: Evaluation status changed: "pending" -&gt; "complete"
==&gt; 2022-07-18T18:04:56+02:00: Evaluation "efe42497" finished with status "complete"
==&gt; 2022-07-18T18:04:56+02:00: Monitoring deployment "577c3e71"
  ✓ Deployment "577c3e71" successful

    2022-07-18T18:04:56+02:00
    ID          = 577c3e71
    Job ID      = todo
    Job Version = 2
    Status      = successful
    Description = Deployment completed successfully

    Deployed
    Task Group  Desired  Placed  Healthy  Unhealthy  Progress Deadline
    web         1        1       1        0          2022-07-18T18:12:24+02:00
</code></pre>

<h4 id="themen-für-fortgeschrittene">Themen für Fortgeschrittene</h4>

<p>Bisher haben wir uns mit den Basics beschäftigt und können jetzt einfache Jobs anlegen, starten
und auch wieder stoppen.
Darauf aufbauend beschäftigen wir uns jetzt im nächsten Abschnitt mit fortgeschrittenen Themen -
alleine schon damit sich der angestrebte Vergleich mit Kubernetes auch sehen lassen kann.</p>

<h5 id="scaling-out">Scaling out</h5>

<p>Für einfache Anwendungsfälle reicht diese einzelne Instanz vollkommen aus, allerdings stoßen wir
damit im Berufsalltag natürlich schnell an eine Grenze.</p>

<p>Bisher haben wir über den Parameter <code>count = 1</code> eine Maximalanzahl von einer Instanz vorgegeben
und vermutlich wird es niemanden überraschen, aber natürlich können hier beliebige Werte eingesetzt
werden - beispielsweise <code>5</code>:</p>

<pre><code class="language-hcl">group "web" {
  count = 5
}
</code></pre>

<p>Da es grundsätzlich nicht schadet, bei allen Änderungen einen <strong>Dry-Run</strong> durchzuführen, und man mit
guten Gewohnheiten nicht früh genug loslegen kann - machen wir das auch direkt:</p>

<p><img src="/assets/images/posts/orchestrierung-mit-nomad/plan_failure.png" alt="image" /></p>

<p>Dieser Fehler kommt jetzt gänzlich unerwartet, denn schließlich haben wir in unserem Job
lediglich <strong>einen</strong> einzelnen statischen Port festgelegt und streben hier ein Deployment von
<strong>fünf</strong> Instanzen auf einem einzelnen Client an.
Dies kann natürlich nicht funktionieren, Nomad weist uns hier folgerichtig auf dieses Problem
hin.</p>

<p>Abhilfe schafft hier <a href="https://www.nomadproject.io/docs/job-specification/network#dynamic-ports=">Dynamic Port Mapping</a>, welches für uns dynamische Ports anlegt und unseren
Instanzen dann automatisch zuweist.</p>

<p>Auf unserer Seite sind dazu lediglich zwei kleinere Anpassungen notwendig:</p>

<ol>
  <li>
    <p>Zunächst entfernen wir unseren statischen Port:</p>

    <pre><code class="language-hcl"> network {
   port "http" {}
 }
</code></pre>
  </li>
  <li>
    <p>Und anschließend teilen wir unserer Quarkus-Anwendung noch mit, unter welchem Port sie jetzt
genau erreichbar ist.
Eine der einfachsten Möglichkeiten ist hier, über <a href="https://en.wikipedia.org/wiki/Environment_variable">Umgebungsvariablen</a> zu arbeiten:</p>

    <pre><code class="language-hcl"> config {
   jar_path = "/U`ers/christoph.kappel/Projects/showcase-nomad-quarkus/target/showcase-nomad-quarkus-0.1-runner.jar"
   jvm_options = [
     "-Xmx256m", "-Xms256m",
     "-Dquarkus.http.port=${NOMAD_PORT_http}" # &lt;1&gt;
   ]
 }
</code></pre>
    <p><strong>&lt;1&gt;</strong> Hier verwenden wir eine <a href="https://developer.hashicorp.com/nomad/docs/runtime/interpolation">Magic Variable</a> von Nomad, die den passenden dynamischen Port enthält.</p>
  </li>
</ol>

<p>Lassen wir nach diesen beiden Änderungen erneut einen <strong>Dry-Run</strong> laufen, sehen wir folgendes:</p>

<p><img src="/assets/images/posts/orchestrierung-mit-nomad/plan_update_scale.png" alt="image" /></p>

<p>Und wenn wir das Deployment schließlich über <strong>Run</strong> ausführen, sehen wir nach ein paar Sekunden
unsere <strong>fünf</strong> Instanzen:</p>

<p><img src="/assets/images/posts/orchestrierung-mit-nomad/update_success.png" alt="image" /></p>

<p>Sinnvollerweise sollten wir jetzt als nächstes irgendeine Art von <a href="https://en.wikipedia.org/wiki/Load_balancing_(computing)">Load-Balancer</a> vor unsere
fünf Instanzen schalten, damit diese erreicht werden können und natürlich die Last gleichmäßig verteilt werden
kann.</p>

<p>Dies bedeutet in den meisten Fällen manuelle Arbeit beim Einrichten der Ports und ein Zusammentragen
der Adressen.
Da es sich hierbei aber auch wieder um ein für uns bereits gelöstes Problem handelt, können wir auf eine
bewährte Lösung zurückgreifen.</p>

<h5 id="service-discovery">Service Discovery</h5>

<p>Bei <a href="https://en.wikipedia.org/wiki/Service_discovery">Service Discovery</a> handelt es sich im Grunde um einen Katalog, bei dem sich Anwendungen mit
ihren bereitgestellten Diensten registrieren und bei dem andere Anwendungen dann Informationen über
bekannte Dienste einholen können.</p>

<p>Hierfür stehen wieder zahlreiche Alternativen zur Verfügung, eine der bekannteren mit hervorragender
Integration und ebenfalls aus dem Hause <a href="https://www.hashicorp.com/">HashiCorp</a> ist <a href="https://www.consul.io/">Consul</a>.</p>

<p>Wir könnten Consul jetzt regulär auf unserem System installieren, aber zur Übung verwenden wir
die <a href="https://www.nomadproject.io/docs/job-specification/artifact#artifact-stanza=">Artifact</a> Stanza und lassen Nomad die Anwendung direkt aus dem Internet laden und
direkt über den <a href="https://www.nomadproject.io/docs/drivers/raw_exec">Raw/Exec</a> Driver ausführen:</p>

<pre><code class="language-hcl">job "consul" {
  datacenters = ["dc1"]

  group "consul" {
    count = 1

    task "consul" {
      driver = "raw_exec" # &lt;1&gt;

      config {
        command = "consul"
        args    = ["agent", "-dev"]
      }

      artifact { # &lt;2&gt;
        source = "https://releases.hashicorp.com/consul/1.12.3/consul_1.12.3_darwin_amd64.zip"
      }
    }
  }
}
</code></pre>
<p><strong>&lt;1&gt;</strong> Zunächst benötigen wir einen neuen Task mit dem Raw/Exec-Driver. <br />
<strong>&lt;2&gt;</strong> Und anschließend legen wir die Source unseres Artefakts fest.</p>

<p>Mittlerweile sollte das Deployment dieses Jobs ziemlich selbsterklärend sein:</p>

<pre><code class="language-bash">$ nomad job run jobs/consul.nomad
==&gt; 2022-07-20T12:15:24+02:00: Monitoring evaluation "eb0330c5"
    2022-07-20T12:15:24+02:00: Evaluation triggered by job "consul"
    2022-07-20T12:15:24+02:00: Evaluation within deployment: "c16677f8"
    2022-07-20T12:15:24+02:00: Allocation "7d9626b8" created: node "68168a84", group "consul"
    2022-07-20T12:15:24+02:00: Evaluation status changed: "pending" -&gt; "complete"
==&gt; 2022-07-20T12:15:24+02:00: Evaluation "eb0330c5" finished with status "complete"
==&gt; 2022-07-20T12:15:24+02:00: Monitoring deployment "c16677f8"
  ✓ Deployment "c16677f8" successful

    2022-07-20T12:15:36+02:00
    ID          = c16677f8
    Job ID      = consul
    Job Version = 0
    Status      = successful
    Description = Deployment completed successfully

    Deployed
    Task Group  Desired  Placed  Healthy  Unhealthy  Progress Deadline
    consul      1        1       1        0          2022-07-20T12:25:34+02:00
</code></pre>

<p>Nach ein paar Sekunden sollte Consul dann gestartet und über folgende URL im Browser aufrufbar sein
<a href="http://localhost:8500">http://localhost:8500</a>:</p>

<p><img src="/assets/images/posts/orchestrierung-mit-nomad/consul_services_nomad.png" alt="image" /></p>

<p>Alle bekannten Services werden dann auf dem <strong>Services</strong>-Reiter aufgeführt - dazu zählen Consul
selbst und Nomad - allerdings leider Fehlanzeige was unsere Instanzen betrifft.</p>

<p>Zum jetzigen Zeitpunkt sind die Services unserer Instanzen für Nomad noch unbekannt und es
müssen weitere Informationen nachgeliefert werden.</p>

<p>Diese liefern wir dann über die <a href="https://www.nomadproject.io/docs/job-specification/service">Service</a> Stanza nach:</p>

<pre><code class="language-hcl">service {
  name = "todo"
  port = "http"

  tags = [
    "urlprefix-/todo", # &lt;1&gt;
  ]

  check { # &lt;2&gt;
    type     = "http"
    path     = "/"
    interval = "2s"
    timeout  = "2s"
  }
}
</code></pre>
<p><strong>&lt;1&gt;</strong> Nomad erlaubt es, <a href="https://developer.hashicorp.com/consul/docs/discovery/services">Tags</a>s zu vergeben, die sich in etwa so verhalten wie <a href="https://developer.hashicorp.com/nomad/docs/job-specification">Label</a> bei
Kubernetes. Was es mit diesem konkreten Tag auf sich hat, erfahrt ihr im nächsten Kapitel.<br />
<strong>&lt;2&gt;</strong> Über das <a href="https://www.nomadproject.io/docs/job-specification/check#check-stanza=">Check</a> Stanza legen wir fest, wie <a href="https://microservices.io/patterns/observability/health-check-api.html">Healthchecks</a> durchführt werden.</p>

<p>Danach führen wir schnell noch einen neuen <strong>Dry-Run</strong> aus, um weitere Überraschungen auszuschließen:</p>

<p><img src="/assets/images/posts/orchestrierung-mit-nomad/plan_update_service.png" alt="image" /></p>

<p>Hier werden dann noch einmal alle Parameter aufgeführt und man bekommt eine Idee, was für weitere
Konfigurationsmöglichkeiten obige Stanza noch mitbringt.</p>

<p>Haben wir alles überprüft, können wir den Job ausführen und sehen dann hoffentlich nach kurzer
Zeit neue Einträge in Consul:</p>

<p><img src="/assets/images/posts/orchestrierung-mit-nomad/consul_services_todo.png" alt="image" /></p>

<blockquote>
  <p><strong><em>NOTE:</em></strong> Diese Übersicht liefert uns dann auch direkt die Port Bindings unserer Instanzen.</p>
</blockquote>

<p>Das ist auch geschafft - fehlt noch Traffic auf unseren Instanzen.</p>

<h5 id="load-balancing">Load-balancing</h5>

<p>Für den nächsten Teil greifen wir abermals auf ein weiteres Tool zurück, da wir hier den
Aufgabenbereich von Nomad verlassen.</p>

<p>Eine der einfachsten Lösungen und mit ebenfalls exzellenter Integration in Nomad und
Consul ist der Proxy <a href="https://fabiolb.net/">Fabio</a>.</p>

<p>In gewohnter Manie können wir uns auch hier zurücklehnen und Nomad die Arbeit überlassen:</p>

<pre><code class="language-hcl">job "fabio" {
  datacenters = ["dc1"]

  group "fabio" {
    count = 1

    task "fabio" {
      driver = "raw_exec"
      config {
        command = "fabio"
        args    = ["-proxy.strategy=rr"] # &lt;1&gt;
      }
      artifact {
        source      = "https://github.com/fabiolb/fabio/releases/download/v1.6.1/fabio-1.6.1-darwin_amd64"
        destination = "local/fabio"
        mode        = "file"
      }
    }
  }
}
</code></pre>
<p><strong>&lt;1&gt;</strong> Hiermit legen wir <a href="https://en.wikipedia.org/wiki/Round-robin_scheduling">Round-Robin</a> als Verteilungsstrategie fest.</p>

<p>Ein kurzer <strong>Dry-Run</strong> gefolgt von einem Deployment und schon sehen wir Fabio in der Liste
der bekannten Services:</p>

<pre><code class="language-bash">$ nomad job plan jobs/fabio.nomad
+ Job: "fabio"
+ Task Group: "fabio" (1 create)
  + Task: "fabio" (forces create)

Scheduler dry-run:
- All tasks successfully allocated.

$ nomad job run jobs/fabio.nomad
==&gt; 2022-07-19T15:53:33+02:00: Monitoring evaluation "eb13753c"
    2022-07-19T15:53:33+02:00: Evaluation triggered by job "fabio"
    2022-07-19T15:53:33+02:00: Allocation "d923c41d" created: node "dd051c02", group "fabio"
==&gt; 2022-07-19T15:53:34+02:00: Monitoring evaluation "eb13753c"
    2022-07-19T15:53:34+02:00: Evaluation within deployment: "2c0db725"
    2022-07-19T15:53:34+02:00: Evaluation status changed: "pending" -&gt; "complete"
==&gt; 2022-07-19T15:53:34+02:00: Evaluation "eb13753c" finished with status "complete"
==&gt; 2022-07-19T15:53:34+02:00: Monitoring deployment "2c0db725"
  ✓ Deployment "2c0db725" successful

    2022-07-19T15:53:46+02:00
    ID          = 2c0db725
    Job ID      = fabio
    Job Version = 0
    Status      = successful
    Description = Deployment completed successfully

    Deployed
    Task Group  Desired  Placed  Healthy  Unhealthy  Progress Deadline
    fabio       1        1       1        0          2022-07-19T16:03:45+02:00
</code></pre>

<p><img src="/assets/images/posts/orchestrierung-mit-nomad/consul_services_fabio.png" alt="image" /></p>

<p>Sprechen wir jetzt Fabio über den Defaultport <code>9999</code> an, bekommen wir erneut die altbekannte
Ausgabe:</p>

<pre><code class="language-bash">$ curl -v -H "Accept: application/json" http://localhost:9999/todo
*   Trying ::1...
* TCP_NODELAY set
* Connected to localhost (::1) port 9999 (#0)
&gt; GET /todo HTTP/1.1
&gt; Host: localhost:9999
&gt; User-Agent: curl/7.64.1
&gt; Accept: application/json
&gt;
&lt; HTTP/1.1 204 No Content
&lt;
* Connection #0 to host localhost left intact
* Closing connection 0
</code></pre>

<p>Wenn wir das ganze jetzt wiederholen, sollte Fabio theoretisch die Last gleichmäßig auf unsere
Instanzen verteilen, allerdings würden wir davon derzeit nichts mitbekommen.</p>

<p>Ein einfacher Trick hier ist, einen neuen <a href="https://en.wikipedia.org/wiki/List_of_HTTP_header_fields">HTTP Header</a> zu setzen, der die entsprechende
IP-Adresse und den Port der Instanz enthält:</p>

<pre><code class="language-hcl">config {
  jar_path = "/Users/christoph.kappel/Projects/showcase-nomad-quarkus/target/showcase-nomad-quarkus-0.1-runner.jar"
  jvm_options = [
    "-Xmx256m", "-Xms256m",
    "-Dquarkus.http.port=${NOMAD_PORT_http}",
    "-Dquarkus.http.header.TodoServer.value=${NOMAD_IP_http}:${NOMAD_PORT_http}", # &lt;1&gt;
    "-Dquarkus.http.header.TodoServer.path=/todo",
    "-Dquarkus.http.header.TodoServer.methods=GET"
  ]
}
</code></pre>
<p><strong>&lt;1&gt;</strong> Wir verwenden hier eine weitere <a href="https://developer.hashicorp.com/nomad/docs/runtime/interpolation">Magic Variable</a> und befüllen damit unseren neuen
<a href="https://en.wikipedia.org/wiki/List_of_HTTP_header_fields">HTTP Header</a>.</p>

<p>Vermutlich könnt ihr euch die nächsten Schritte denken, daher das ganze im Schnelldurchlauf:</p>

<pre><code class="language-bash">$ nomad job plan jobs/todo-java-scaled-service-header.nomad
+/- Job: "todo"
+/- Task Group: "web" (1 create/destroy update, 4 ignore)
  +/- Task: "todo" (forces create/destroy update)
    +/- Config {
        jar_path:       "/Users/christoph.kappel/Projects/showcase-nomad-quarkus/target/showcase-nomad-quarkus-0.1-runner.jar"
        jvm_options[0]: "-Xmx256m"
        jvm_options[1]: "-Xms256m"
        jvm_options[2]: "-Dquarkus.http.port=${NOMAD_PORT_http}"
      + jvm_options[3]: "-Dquarkus.http.header.TodoServer.value=${NOMAD_IP_http}:${NOMAD_PORT_http}"
      + jvm_options[4]: "-Dquarkus.http.header.TodoServer.path=/todo"
      + jvm_options[5]: "-Dquarkus.http.header.TodoServer.methods=GET"
        }

$ nomad job run jobs/todo-java-scaled-service-header.nomad
==&gt; 2022-07-20T17:03:39+02:00: Monitoring evaluation "909df36e"
    2022-07-20T17:03:39+02:00: Evaluation triggered by job "todo"
==&gt; 2022-07-20T17:03:40+02:00: Monitoring evaluation "909df36e"
    2022-07-20T17:03:40+02:00: Evaluation within deployment: "409e814e"
    2022-07-20T17:03:40+02:00: Allocation "03e95d99" created: node "9293fb2f", group "web"
    2022-07-20T17:03:40+02:00: Evaluation status changed: "pending" -&gt; "complete"
==&gt; 2022-07-20T17:03:40+02:00: Evaluation "909df36e" finished with status "complete"
==&gt; 2022-07-20T17:03:40+02:00: Monitoring deployment "409e814e"
  ✓ Deployment "409e814e" successful

    2022-07-21T14:38:50+02:00
    ID          = 409e814e
    Job ID      = todo
    Job Version = 2
    Status      = successful
    Description = Deployment completed successfully

    Deployed
    Task Group  Desired  Placed  Healthy  Unhealthy  Progress Deadline
    web         5        5       5        0          2022-07-20T17:14:49+02:00

Scheduler dry-run:
- All tasks successfully allocated.
</code></pre>

<p>Testen wir das ganze nach erfolgreichem Deployment, sehen wir, wie die verschiedenen
Instanzen der Reihe nach angesprochen werden:</p>

<p><img src="/assets/images/posts/orchestrierung-mit-nomad/loadbalancer.gif" alt="image" /></p>

<p>Falls ihr euch jetzt fragt weshalb, das ganze überhaupt ohne weitere Konfiguration funktioniert:</p>

<p>Einer der großen Vorteile von Fabio ist, Routen können über <a href="https://developer.hashicorp.com/consul/docs/discovery/services">Service Tags</a> festgelegt werden.
Und schaut ihr jetzt genau hin, haben wir dies in unseren Beispielen auch schon gemacht:
<code>urlprefix-/todo</code></p>

<p>Über diesen Service Tag teilen wir Fabio mit, wie es mit Requests für die Route <code>/todo</code>
umgehen und anhand der konfigurierten Strategie verteilen soll.</p>

<blockquote>
  <p><strong><em>NOTE:</em></strong> Weitere Konfigurationsmöglichkeiten findet ihr im entsprechenden <a href="https://fabiolb.net/quickstart/">Quickstart Guide</a>.</p>
</blockquote>

<h5 id="update-strategien">Update Strategien</h5>

<p>Unsere Anwendung läuft jetzt erfolgreich auf einem einzelnen Client und wir haben sowohl die
Ausfallwahrscheinlichkeit reduziert als auch Lastverteilung eingeführt, indem wir unsere fünf
Instanzen in einem gemeinsamen <a href="https://en.wikipedia.org/wiki/Load_balancing_(computing)">Load-Balancer</a>-Verbund zusammengefasst haben.</p>

<p>Soweit, so gut - aber wie können wir hier jetzt am geschicktesten Updates durchführen?</p>

<p>Grundsätzlich stehen dafür verschiedene Ansätze zur Verfügung und der einfachste ist natürlich,
alle Instanzen in einem Rutsch zu aktualisieren.</p>

<blockquote>
  <p><strong><em>NOTE:</em></strong> Hier spricht man auch von <a href="https://en.wikipedia.org/wiki/Batch_processing">Batch-Size</a> und diese wäre in unserem Beispiel <code>5</code>.</p>
</blockquote>

<p>Wählen wir diesen Ansatz, negieren wir vermutlich einige der vorhin angesprochenen Vorteile und
enden im Fehlerfall möglicherweise mit einer zu geringen Anzahl an Instanzen für unseren Workload.
Ein besserer Ansatz hier ist, eine möglichst kleine Batch-Size zu wählen und die Instanzen
rollierend - sprich nach und nach - zu aktualisieren.</p>

<p>Standardmäßig führt Nomad ein <a href="https://en.wikipedia.org/wiki/Rolling_release">Rolling Update</a> durch, allerdings lassen sich viele weitere
Strategien über das <a href="https://www.nomadproject.io/docs/job-specification/update">Update</a> Stanza realisieren.</p>

<pre><code class="language-hcl">update {
  canary       = 1 # &lt;1&gt;
  max_parallel = 5 # &lt;2&gt;
}
</code></pre>
<p><strong>&lt;1&gt;</strong> Dies legt die Anzahl der Instanzen fest, die in einem <a href="https://martinfowler.com/bliki/CanaryRelease.html">Canary Update</a> aktualisiert werden.<br />
<strong>&lt;2&gt;</strong> Und hiermit wird die Batch-Size des Updates festgelegt.</p>

<p>Bevor wir jetzt als einfaches Beispiel ein Canary-Update durchführen, sollten wir noch einmal
kurz in uns gehen und überlegen, wie wir sicherstellen können, dass ein Update den gewünschten
Erfolg gebracht hat.</p>

<p>Ein Canary-Update mit <code>canary = 1</code> bedeutet, unser Orchestrator startet eine neue Instanz und
wartet anschließend auf die Bestätigung, fortzufahren.
Bei unserer Aufgabe zu prüfen, ob diese neue Instanz ordnungsgemäß funktioniert, laufen
aber natürlich wieder in die altbekannte Problematik der zufälligen Zuweisung durch den
Load-Balancer.</p>

<p>Vorhin hat der Trick mit dem <a href="https://en.wikipedia.org/wiki/List_of_HTTP_header_fields">HTTP Header</a> hervorragend geklappt - nutzen wir die Idee also
erneut:</p>

<pre><code class="language-hcl">config {
  jar_path = "/Users/christoph.kappel/Projects/showcase-nomad-quarkus/target/showcase-nomad-quarkus-0.1-runner.jar"
  jvm_options = [
    "-Xmx256m", "-Xms256m",
    "-Dquarkus.http.port=${NOMAD_PORT_http}",
    "-Dquarkus.http.header.TodoServer.value=${NOMAD_IP_http}:${NOMAD_PORT_http}",
    "-Dquarkus.http.header.TodoServer.path=/todo",
    "-Dquarkus.http.header.TodoServer.methods=GET",
    "-Dquarkus.http.header.TodoServerCanary.value=yes", # &lt;1&gt;
    "-Dquarkus.http.header.TodoServer.path=/todo",
    "-Dquarkus.http.header.TodoServer.methods=GET"
  ]
}
</code></pre>
<p><strong>&lt;1&gt;</strong> Unser neuer <a href="https://en.wikipedia.org/wiki/List_of_HTTP_header_fields">HTTP Header</a> für das Update.</p>

<p>Wir führen erneut einen <strong>Dry-Run</strong> gefolgt von einem Deployment aus:</p>

<pre><code class="language-bash">$ nomad job plan jobs/todo-java-scaled-service-header-canary.nomad
+/- Job: "todo"
+/- Task Group: "web" (1 canary, 5 ignore)
  +/- Update {
        AutoPromote:      "false"
        AutoRevert:       "false"
    +/- Canary:           "0" =&gt; "1"
        HealthCheck:      "checks"
        HealthyDeadline:  "300000000000"
    +/- MaxParallel:      "1" =&gt; "5"
        MinHealthyTime:   "10000000000"
        ProgressDeadline: "600000000000"
      }
  +/- Task: "todo" (forces create/destroy update)
    +/- Config {
        jar_path:       "/Users/christoph.kappel/Projects/showcase-nomad-quarkus/target/showcase-nomad-quarkus-0.1-runner.jar"
        jvm_options[0]: "-Xmx256m"
        jvm_options[1]: "-Xms256m"
        jvm_options[2]: "-Dquarkus.http.port=${NOMAD_PORT_http}"
        jvm_options[3]: "-Dquarkus.http.header.TodoServer.value=${NOMAD_IP_http}:${NOMAD_PORT_http}"
        jvm_options[4]: "-Dquarkus.http.header.TodoServer.path=/todo"
        jvm_options[5]: "-Dquarkus.http.header.TodoServer.methods=GET"
      + jvm_options[6]: "-Dquarkus.http.header.TodoServerCanary.value=yes"
      + jvm_options[7]: "-Dquarkus.http.header.TodoServer.path=/todo"
      + jvm_options[8]: "-Dquarkus.http.header.TodoServer.methods=GET"
        }

Scheduler dry-run:
- All tasks successfully allocated.

$ nomad job run jobs/todo-java-scaled-service-header-canary.nomad
==&gt; 2022-07-20T17:11:53+02:00: Monitoring evaluation "43bdfab2"
    2022-07-20T17:11:53+02:00: Evaluation triggered by job "todo"
    2022-07-20T17:11:53+02:00: Allocation "4963b7fc" created: node "9293fb2f", group "web"
==&gt; 2022-07-20T17:11:54+02:00: Monitoring evaluation "43bdfab2"
    2022-07-20T17:11:54+02:00: Evaluation within deployment: "a0c1e782"
    2022-07-20T17:11:54+02:00: Allocation "4963b7fc" status changed: "pending" -&gt; "running" (Tasks are running)
    2022-07-20T17:11:54+02:00: Evaluation status changed: "pending" -&gt; "complete"
==&gt; 2022-07-20T17:11:54+02:00: Evaluation "43bdfab2" finished with status "complete"
==&gt; 2022-07-20T17:11:54+02:00: Monitoring deployment "a0c1e782"
  ⠇ Deployment "a0c1e782" in progress...

    2022-07-21T15:12:10+02:00
    ID          = a0c1e782
    Job ID      = todo
    Job Version = 6
    Status      = running
    Description = Deployment is running but requires manual promotion

    Deployed
    Task Group  Promoted  Desired  Canaries  Placed  Healthy  Unhealthy  Progress Deadline
    web         false     5        1         1       1        0          2022-07-20T17:22:06+02:00
</code></pre>

<p>Der spannende Teil hier ist, dass Nomad das Deployment unterbricht und wir die nötige Zeit
bekommen, um zu prüfen, ob unsere neue Version ordnungsgemäß funktioniert:</p>

<p><img src="/assets/images/posts/orchestrierung-mit-nomad/canary.gif" alt="image" /></p>

<p>Haben wir uns davon ausreichend überzeugt, können wir Nomad anweisen, das Deployment fortzusetzen
und auch die verbleibenden Instanzen auszurollen.
Hierfür stehen uns wie bisher verschiedene Wege zur Verfügung, allerdings ist dies im Webinterface
sehr anschaulich:</p>

<p><img src="/assets/images/posts/orchestrierung-mit-nomad/promote_canary.png" alt="image" /></p>

<p>Mit <strong>Promote Canary</strong> setzen wir das unterbrochene Deployment dann schließlich fort:</p>

<p><img src="/assets/images/posts/orchestrierung-mit-nomad/promote_canary_success.png" alt="image" /></p>

<h4 id="fazit">Fazit</h4>

<p>Nomad ist ein einfacher und flexibler Job-Scheduler, der durch die Integration weiterer
Produkte praxistaugliche Lösungen für gängige Probleme liefert und sich somit in keiner Weise
hinter dem großen Bruder Kubernetes verstecken muss.</p>

<p>Sämtliche Beispiele dieses Beitrags könnt ihr in folgendem Repository einsehen:</p>

<p><a href="https://github.com/unexist/showcase-nomad-quarkus">https://github.com/unexist/showcase-nomad-quarkus</a></p>


</div>
</div>
</div>
</div>]]></field>
    </document>
</documents>
